import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, average_precision_score
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from sklearn.ensemble import RandomForestClassifier
from imblearn.combine import SMOTETomek
import joblib
import matplotlib.pyplot as plt
import warnings
import optuna
from sklearn.feature_selection import SelectKBest, f_classif

warnings.filterwarnings('ignore')

def load_processed_data(file_path='../æ±äº¬AMIå¤©å€™å…¥é™¢äººæ•°è¾¼ã¿ãƒ¢ãƒ‡ãƒ«å¤©æ°—è©³ç´°è¿½åŠ å¾Œ/æ±äº¬AMIå¤©æ°—ãƒ‡ãƒ¼ã‚¿ã¨JROADçµåˆå¾Œ2012å¹´4æœˆ1æ—¥ã‹ã‚‰2021å¹´12æœˆ31æ—¥å¤©æ°—æ¦‚æ³æ•´ç†.csv'):
    """å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    from data_preprocessing import load_data, preprocess_data
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
    df = load_data(file_path)
    processed_df, feature_columns = preprocess_data(df)
    
    return processed_df, feature_columns

def create_time_based_splits(df, n_splits=5):
    """æ™‚ç³»åˆ—ã«åŸºã¥ãè¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²"""
    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=int(len(df) * 0.2))
    return tscv.split(df)

def train_xgboost(X_train, y_train, X_val, y_val):
    """XGBoostãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'max_depth': 3,
        'learning_rate': 0.01,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'min_child_weight': 3,
        'reg_alpha': 0.1,
        'reg_lambda': 1.0,
        'n_estimators': 100
    }
    
    model = xgb.XGBClassifier(**params)
    model.fit(X_train, y_train)
    
    return model

def train_lightgbm(X_train, y_train, X_val, y_val):
    """LightGBMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'max_depth': 3,
        'learning_rate': 0.01,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'min_child_samples': 20,
        'reg_alpha': 0.1,
        'reg_lambda': 1.0,
        'n_estimators': 100
    }
    
    model = lgb.LGBMClassifier(**params)
    model.fit(X_train, y_train)
    
    return model

def train_random_forest(X_train, y_train):
    """ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
    params = {
        'n_estimators': 100,
        'max_depth': 3,
        'min_samples_split': 5,
        'min_samples_leaf': 3,
        'max_features': 'sqrt',
        'random_state': 42
    }
    
    model = RandomForestClassifier(**params)
    model.fit(X_train, y_train)
    
    return model

def evaluate_model(model, X_test, y_test, model_name):
    """ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"""
    # äºˆæ¸¬ç¢ºç‡ã®å–å¾—
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    y_pred = (y_pred_proba > 0.5).astype(int)
    
    # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    pr_auc = average_precision_score(y_test, y_pred_proba)
    
    print(f"\n{model_name} Performance:")
    print(f"ROC-AUC Score: {roc_auc:.4f}")
    print(f"PR-AUC Score: {pr_auc:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
    
    return roc_auc, pr_auc

def plot_feature_importance(model, feature_names, model_name):
    """ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
    elif hasattr(model, 'get_score'):
        importances = model.get_score(importance_type='gain')
    else:
        return
    
    # é‡è¦åº¦ã®é™é †ã§ã‚½ãƒ¼ãƒˆ
    indices = np.argsort(importances)[::-1]
    
    plt.figure(figsize=(10, 6))
    plt.title(f'Feature Importances ({model_name})')
    plt.bar(range(len(importances)), importances[indices])
    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(f'../results/{model_name.lower()}_feature_importance.png')
    plt.close()

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã‚’é–‹å§‹...")
    df, feature_columns = load_processed_data()
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®æº–å‚™
    X = df[feature_columns]
    y = df['target']
    
    print(f"X shape: {X.shape}")
    print(f"y shape: {y.shape}")
    print(f"y unique values: {y.unique()}")
    print(f"Feature columns count: {len(feature_columns)}")
    
    # ãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
    print(f"X missing values: {X.isnull().sum().sum()}")
    print(f"y missing values: {y.isnull().sum()}")
    
    # æ™‚ç³»åˆ—åˆ†å‰²ã®ä½œæˆ
    splits = create_time_based_splits(df)
    
    # ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡çµæœã‚’ä¿å­˜
    results = []
    
    # å„åˆ†å‰²ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ãƒ»è©•ä¾¡
    for fold, (train_idx, test_idx) in enumerate(splits, 1):
        print(f"\nFold {fold}")
        
        # ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
        
        print(f"X_train shape: {X_train.shape}")
        print(f"y_train shape: {y_train.shape}")
        print(f"y_train unique: {y_train.unique()}")
        
        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æœ€å¾Œã®20%ï¼‰
        val_size = int(len(X_train) * 0.2)
        X_val = X_train[-val_size:]
        y_val = y_train[-val_size:]
        X_train = X_train[:-val_size]
        y_train = y_train[:-val_size]
        
        print(f"After split - X_train: {X_train.shape}, y_train: {y_train.shape}")
        print(f"After split - X_val: {X_val.shape}, y_val: {y_val.shape}")
        
        # å„ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡
        models = {
            'XGBoost': train_xgboost(X_train, y_train, X_val, y_val),
            'LightGBM': train_lightgbm(X_train, y_train, X_val, y_val),
            'RandomForest': train_random_forest(X_train, y_train)
        }
        
        # å„ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
        fold_results = {}
        for name, model in models.items():
            roc_auc, pr_auc = evaluate_model(model, X_test, y_test, name)
            fold_results[name] = {'roc_auc': roc_auc, 'pr_auc': pr_auc}
            
            # ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
            plot_feature_importance(model, feature_columns, f"{name}_fold{fold}")
        
        results.append(fold_results)
    
    # å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®å¹³å‡æ€§èƒ½ã‚’è¨ˆç®—
    print("\nAverage Performance Across Folds:")
    for model_name in ['XGBoost', 'LightGBM', 'RandomForest']:
        avg_roc_auc = np.mean([fold[model_name]['roc_auc'] for fold in results])
        avg_pr_auc = np.mean([fold[model_name]['pr_auc'] for fold in results])
        print(f"\n{model_name}:")
        print(f"Average ROC-AUC: {avg_roc_auc:.4f}")
        print(f"Average PR-AUC: {avg_pr_auc:.4f}")
    
    # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ï¼‰
    final_models = {
        'XGBoost': train_xgboost(X, y, X_val, y_val),
        'LightGBM': train_lightgbm(X, y, X_val, y_val),
        'RandomForest': train_random_forest(X, y)
    }
    
    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®é¸æŠã¨ä¿å­˜
    best_model_name = None
    best_auc = 0
    
    for name, model in final_models.items():
        # å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
        y_pred_proba = model.predict_proba(X)[:, 1]
        auc = roc_auc_score(y, y_pred_proba)
        
        if auc > best_auc:
            best_auc = auc
            best_model_name = name
        
        print(f"{name} Final AUC: {auc:.4f}")
    
    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    best_model = final_models[best_model_name]
    joblib.dump(best_model, f'auc0.83_weather_flags_model_{best_auc:.4f}.pkl')
    
    print(f"\næœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model_name} (AUC: {best_auc:.4f})")
    print("ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    
    return best_model, best_auc

if __name__ == "__main__":
    best_model, best_auc = main()
    print(f"\nğŸ† æœ€çµ‚çµæœ: AUC {best_auc:.4f}") 